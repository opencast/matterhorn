###########################################
### Matterhorn configuration properties ###
###########################################

# The HTTP server port.  If you set this to port 80, you need to run Matterhorn as root. Alternatively, if you want
# users to access Matterhorn on port 80 but do not want to run as root, keep the default port (8080) and use an Apache
# HTTPD server with mod_proxy to forward port 80 traffic to Matterhorn on port 8080.
org.osgi.service.http.port=8080

# Whether Matterhorn itself should handle HTTPS traffic.  Even if you set this to 'false',you can still use an Apache
# HTTPD server as a proxy to handle SSL)
org.osgi.service.http.secure.enabled=false

# The secure server port to use if running Matterhorn itself with HTTPS (as opposed to a proxy handling HTTPS).
#org.osgi.service.http.port.secure=8443

# The public URL of this matterhorn installation.  If this felix installation is proxied behind an Apache HTTPD
# reverse proxy, this URL should point to the proxy's port (usually 80).
org.opencastproject.server.url=http://localhost:8080

# The base URL of the server hosting the administrative tools.  If the admin tools are deployed on this server,
# this should point to this server's public URL.
org.opencastproject.admin.ui.url=${org.opencastproject.server.url}

# The base URL of the server hosting the engage tools.  If the engage tools are deployed on this server, this
# should point to this server's public URL.
org.opencastproject.engage.ui.url=${org.opencastproject.server.url}

# The url for the service registry, which will allow distributed installations to find remote service instances.
#org.opencastproject.serviceregistry.url=${org.opencastproject.server.url}/serviceregistry/rest

# The directory where the system will store its processed files (including temporary files).  This directory should
# be persistent between reboots (i.e., not /tmp)
org.opencastproject.storage.dir=${java.io.tmpdir}/opencast

# The path to the security configuration.  If using a relative path, be sure to start Matterhorn from the correct
# directory.  An absolute path is recommended.
org.opencastproject.security.config=conf/security.xml

# The username and password to present to other Matterhorn servers when calling their REST endpoints.
# The remote server must contain an entry for this username and password in its security configuration.
org.opencastproject.security.digest.user=matterhorn_system_account
org.opencastproject.security.digest.pass=CHANGE_ME

# The base URL of the streaming server (ususally "rtmp://<SERVER_URL>/matterhorn-engage").
# ${org.opencastproject.server.url} can not be used, because the streaming server does not use the HTTP protocol.
# Streaming is not included in the default workflow, since the Red5 streaming server is a 3rd party component that
# requires separate installation.
#org.opencastproject.streaming.url=rtmp://localhost/matterhorn-engage

# The directory where the matterhorn streaming app for Red5 stores the streams
#org.opencastproject.streaming.directory=${org.opencastproject.storage.dir}/streams

# The directory to store media, metadata, and attachments for download from the engage tool
org.opencastproject.download.directory=${org.opencastproject.storage.dir}/downloads

# The base URL for media downloads.
org.opencastproject.download.url=${org.opencastproject.server.url}/static

# Relational Database configuration.  By default, Matterhorn uses an embedded H2 database.  A standalone database server
# is recommended for production systems.  If you run the ddl script for your db vendor (see docs/scripts/ddl/) manually,
# (this is recommended) set 'ddl-generation' to 'false'.
org.opencastproject.db.ddl.generation=true

# dbVendor can be any of the values listed at under the "eclipselink.target-database" section of
# http://wiki.eclipse.org/Using_EclipseLink_JPA_Extensions_%28ELUG%29
# org.opencastproject.db.vendor=

# Matterhorn comes with the jdbc drivers for MySQL (com.mysql.jdbc.Driver) and PostgreSQL (org.postgresql.Driver). To
# add other jdbcDrivers to the Matterhorn runtime, rebuild the matterhorn-db module with your desired drivers.
# org.opencastproject.db.jdbc.driver=

# The jdbc connection url, username, and password
# org.opencastproject.db.jdbc.url=
# org.opencastproject.db.jdbc.user=
# org.opencastproject.db.jdbc.pass=

# The path for the ffmpeg binary in the ffmpeg encoder (default: /usr/local/bin/ffmpeg) 
#org.opencastproject.composer.ffmpegpath=/usr/local/bin/ffmpeg

# Configuration for the org.opencastproject.analysis.text.ocropus.OcropusTextAnalyzer binary (default: /usr/local/bin/ocrocmd)
#org.opencastproject.textanalyzer.ocrocmd= /usr/local/bin/ocrocmd

# The path for the qtsbtlembedder binary for QuickTime subtitle embedder (default: /usr/local/bin/qtsbtlembedder)
#org.opencastproject.composer.qtembedderpath=/usr/local/bin/qtsbtlembedder

# Class to use for analyzing media (default: org.opencastproject.inspection.impl.MediaInfoAnalyzer)
#inspection.analyzerclass=org.opencastproject.inspection.impl.MediaInfoAnalyzer
# Configuration for the org.opencastproject.inspection.impl.MediaInfoAnalyzer binary (default: /usr/local/bin/mediainfo)
#org.opencastproject.inspection.mediainfopath=/usr/local/bin/mediainfo

# Directory to store the search index.  This should be a persistent and stable directory (default:
# ${org.opencastproject.storage.dir}/searchindex)
#org.opencastproject.search.solr.dir=${org.opencastproject.storage.dir}/searchindex

# Url of the dedicated Solr server for use with the search service.  Note that if the url is specified, the local search
# index as configured using ${org.opencastproject.search.solr.dir} will be ignored. A dedicated Solr server should be
# set up in order to enable running multiple instances of the search service. Please consult
# http://lucene.apache.org/solr/ on how to set up a standalone Solr server.
#org.opencastproject.search.solr.url=http://localhost:8983/solr/

# The path to the repository of files used during media processing.
org.opencastproject.file.repo.path=${org.opencastproject.storage.dir}/files

# The base URL of the file server.  When using a shared filesystem between servers, set all servers to use the same URL.
# If this is set to any value other than ${org.opencastproject.admin.ui.url}, the admin and file servers must use a
# single sign on (SSO) solution such as CAS to avoid requiring users to perform multiple logins.
#org.opencastproject.file.repo.url=${org.opencastproject.admin.ui.url}

# The path to the working files (recommend using fast, transient storage)
org.opencastproject.workspace.rootdir=${org.opencastproject.storage.dir}/workspace

# The ID of the default workflow definition to run when media are ingested
org.opencastproject.workflow.default.definition=full

# The directory to hold the workflow service's solr configuration and data
org.opencastproject.workflow.solr.dir=${org.opencastproject.storage.dir}/workflow

# Url of the dedicated Solr server to use with the workflow service.  Note that if the url is specified, the local
# workflow index as configured using ${org.opencastproject.workflow.solr.dir} will be ignored. A dedicated Solr server
# should be set up in order to enable running multiple instances of the workflow service. Please consult
# http://lucene.apache.org/solr/ on how to set up a standalone Solr server.
#org.opencastproject.workflow.solr.url=http://localhost:8983/solr/

# Send server configuration data to the opencast project to help us track how people are using Matterhorn.  No security
# related information will be sent to the opencast project.  Comment this out to disable this feature.
org.opencastproject.anonymous.feedback.url=http://www.opencastproject.org/form/tracking

# Display UI test suites and resources (for development only)
testMode=true

# The maximum number of concurrent files to ingest from the inbox directory
#org.opencastproject.inbox.threads=1

# The maximum number of concurrent encoding jobs to execute.  Any jobs submitted beyond this limit will wait in a queue
# until a composer thread is available.
#org.opencastproject.composer.threads=1

# The maximum number of concurrent video segmentation jobs to execute.  Any jobs submitted beyond this limit will wait
# in a queue until a video segmenter thread is available.
#org.opencastproject.videosegmenter.threads=1

# The maximum number of concurrent text analyzer jobs to execute.  Any jobs submitted beyond this limit will wait in a
# queue until a text analyzer thread is available.
#org.opencastproject.textanalyzer.threads=1

# The maximum number of concurrent download distribution jobs to execute.  Any jobs submitted beyond this limit will wait in a
# queue until a distribution thread is available.
#org.opencastproject.distribution.download.threads=1

# The maximum number of concurrent streaming distribution jobs to execute.  Any jobs submitted beyond this limit will wait in a
# queue until a distribution thread is available.
#org.opencastproject.distribution.streaming.threads=1

# The maximum number of concurrent itunesu distribution jobs to execute.  Any jobs submitted beyond this limit will wait in a
# queue until a distribution thread is available.
#org.opencastproject.distribution.itunesu.threads=1

# The maximum number of concurrent youtube distribution jobs to execute.  Any jobs submitted beyond this limit will wait in a
# queue until a distribution thread is available.
#org.opencastproject.distribution.youtube.threads=1

# The maximum number of concurrent caption converter jobs to execute.  Any jobs submitted beyond this limit will wait in a
# queue until a thread is available.
#org.opencastproject.captionconverter.threads=1

# The maximum number of concurrent media inspection jobs to execute.  Any jobs submitted beyond this limit will wait in a
# queue until a thread is available.
#org.opencastproject.mediainspection.threads=1

# The maximum number of concurrent file zipping jobs to execute.  Any jobs submitted beyond this limit will wait
# in a queue until an archiving thread is available.
#org.opencastproject.zip.threads=1
